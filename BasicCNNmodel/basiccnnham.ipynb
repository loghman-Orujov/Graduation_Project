{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38225329",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-11T08:04:30.831317Z",
     "iopub.status.busy": "2025-06-11T08:04:30.831044Z",
     "iopub.status.idle": "2025-06-11T16:35:43.243705Z",
     "shell.execute_reply": "2025-06-11T16:35:43.242654Z"
    },
    "papermill": {
     "duration": 30672.425893,
     "end_time": "2025-06-11T16:35:43.251710",
     "exception": false,
     "start_time": "2025-06-11T08:04:30.825817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Fold 1/2\n",
      "\n",
      "Train set class distribution:\n",
      "Counter({1: 490, 0: 374, 2: 227})\n",
      "\n",
      "Validation set class distribution:\n",
      "Counter({1: 491, 0: 374, 2: 226})\n",
      "\n",
      "Class weights for this fold: [0.861496877967819, 1.2657234296218451, 0.9860886199974187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Training: 100%|██████████| 137/137 [07:51<00:00,  3.44s/it]\n",
      "Epoch 1/25 - Validation: 100%|██████████| 137/137 [09:38<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:\n",
      "Train Loss: 1.0259, Train Acc: 0.5060\n",
      "Val Loss: 1.1031, Val Acc: 0.4500\n",
      "--Yeni en iyi model kaydedildi--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Training: 100%|██████████| 137/137 [06:52<00:00,  3.01s/it]\n",
      "Epoch 2/25 - Validation: 100%|██████████| 137/137 [08:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25:\n",
      "Train Loss: 0.9554, Train Acc: 0.5490\n",
      "Val Loss: 1.0692, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Training: 100%|██████████| 137/137 [06:22<00:00,  2.79s/it]\n",
      "Epoch 3/25 - Validation: 100%|██████████| 137/137 [09:29<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25:\n",
      "Train Loss: 0.9612, Train Acc: 0.5344\n",
      "Val Loss: 1.1235, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Training: 100%|██████████| 137/137 [06:26<00:00,  2.82s/it]\n",
      "Epoch 4/25 - Validation: 100%|██████████| 137/137 [09:06<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25:\n",
      "Train Loss: 0.9645, Train Acc: 0.5344\n",
      "Val Loss: 1.0845, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Training: 100%|██████████| 137/137 [06:07<00:00,  2.68s/it]\n",
      "Epoch 5/25 - Validation: 100%|██████████| 137/137 [09:14<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25:\n",
      "Train Loss: 0.9764, Train Acc: 0.5206\n",
      "Val Loss: 1.0644, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Training: 100%|██████████| 137/137 [06:14<00:00,  2.73s/it]\n",
      "Epoch 6/25 - Validation: 100%|██████████| 137/137 [09:03<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25:\n",
      "Train Loss: 0.9593, Train Acc: 0.5380\n",
      "Val Loss: 1.0830, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Training: 100%|██████████| 137/137 [06:05<00:00,  2.66s/it]\n",
      "Epoch 7/25 - Validation: 100%|██████████| 137/137 [08:57<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25:\n",
      "Train Loss: 0.9328, Train Acc: 0.5536\n",
      "Val Loss: 1.1425, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Training: 100%|██████████| 137/137 [06:24<00:00,  2.80s/it]\n",
      "Epoch 8/25 - Validation: 100%|██████████| 137/137 [09:06<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25:\n",
      "Train Loss: 0.9698, Train Acc: 0.5261\n",
      "Val Loss: 1.0941, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Training: 100%|██████████| 137/137 [06:29<00:00,  2.84s/it]\n",
      "Epoch 9/25 - Validation: 100%|██████████| 137/137 [09:23<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25:\n",
      "Train Loss: 0.9617, Train Acc: 0.5234\n",
      "Val Loss: 1.0964, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Training: 100%|██████████| 137/137 [06:54<00:00,  3.02s/it]\n",
      "Epoch 10/25 - Validation: 100%|██████████| 137/137 [09:15<00:00,  4.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25:\n",
      "Train Loss: 0.9746, Train Acc: 0.5096\n",
      "Val Loss: 1.0981, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Training: 100%|██████████| 137/137 [06:50<00:00,  2.99s/it]\n",
      "Epoch 11/25 - Validation: 100%|██████████| 137/137 [09:25<00:00,  4.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25:\n",
      "Train Loss: 0.9719, Train Acc: 0.5115\n",
      "Val Loss: 1.1110, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Training: 100%|██████████| 137/137 [07:01<00:00,  3.08s/it]\n",
      "Epoch 12/25 - Validation: 100%|██████████| 137/137 [09:11<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25:\n",
      "Train Loss: 0.9470, Train Acc: 0.5490\n",
      "Val Loss: 1.1090, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Training: 100%|██████████| 137/137 [06:37<00:00,  2.90s/it]\n",
      "Epoch 13/25 - Validation: 100%|██████████| 137/137 [09:02<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25:\n",
      "Train Loss: 1.0011, Train Acc: 0.4840\n",
      "Val Loss: 1.0693, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Training: 100%|██████████| 137/137 [06:51<00:00,  3.00s/it]\n",
      "Epoch 14/25 - Validation: 100%|██████████| 137/137 [09:07<00:00,  4.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25:\n",
      "Train Loss: 0.9552, Train Acc: 0.5445\n",
      "Val Loss: 1.1760, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Training: 100%|██████████| 137/137 [06:28<00:00,  2.84s/it]\n",
      "Epoch 15/25 - Validation: 100%|██████████| 137/137 [08:50<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25:\n",
      "Train Loss: 0.9630, Train Acc: 0.5335\n",
      "Val Loss: 1.0712, Val Acc: 0.4500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Training: 100%|██████████| 137/137 [06:39<00:00,  2.91s/it]\n",
      "Epoch 16/25 - Validation: 100%|██████████| 137/137 [08:59<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25:\n",
      "Train Loss: 0.9565, Train Acc: 0.5353\n",
      "Val Loss: 1.0702, Val Acc: 0.4500\n",
      "Early stopping triggered after 16 epochs\n",
      "Test değerlendirme hatası: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n",
      "\n",
      "Fold 2/2\n",
      "\n",
      "Train set class distribution:\n",
      "Counter({1: 491, 0: 374, 2: 226})\n",
      "\n",
      "Validation set class distribution:\n",
      "Counter({1: 490, 0: 374, 2: 227})\n",
      "\n",
      "Class weights for this fold: [0.8606191427649136, 1.2685206118833314, 0.9860886199974187]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Training: 100%|██████████| 137/137 [08:31<00:00,  3.73s/it]\n",
      "Epoch 1/25 - Validation: 100%|██████████| 137/137 [08:43<00:00,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25:\n",
      "Train Loss: 1.0000, Train Acc: 0.5445\n",
      "Val Loss: 1.2405, Val Acc: 0.4491\n",
      "--Yeni en iyi model kaydedildi--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25 - Training: 100%|██████████| 137/137 [06:57<00:00,  3.05s/it]\n",
      "Epoch 2/25 - Validation: 100%|██████████| 137/137 [08:56<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/25:\n",
      "Train Loss: 0.9812, Train Acc: 0.5252\n",
      "Val Loss: 1.0947, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25 - Training: 100%|██████████| 137/137 [07:30<00:00,  3.29s/it]\n",
      "Epoch 3/25 - Validation: 100%|██████████| 137/137 [08:47<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/25:\n",
      "Train Loss: 0.9833, Train Acc: 0.5151\n",
      "Val Loss: 1.0765, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25 - Training: 100%|██████████| 137/137 [06:37<00:00,  2.90s/it]\n",
      "Epoch 4/25 - Validation: 100%|██████████| 137/137 [09:03<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/25:\n",
      "Train Loss: 0.9623, Train Acc: 0.5325\n",
      "Val Loss: 1.0970, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25 - Training: 100%|██████████| 137/137 [06:36<00:00,  2.90s/it]\n",
      "Epoch 5/25 - Validation: 100%|██████████| 137/137 [10:21<00:00,  4.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/25:\n",
      "Train Loss: 0.9248, Train Acc: 0.5637\n",
      "Val Loss: 1.0984, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25 - Training: 100%|██████████| 137/137 [06:39<00:00,  2.91s/it]\n",
      "Epoch 6/25 - Validation: 100%|██████████| 137/137 [09:21<00:00,  4.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/25:\n",
      "Train Loss: 0.9367, Train Acc: 0.5518\n",
      "Val Loss: 1.1237, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25 - Training: 100%|██████████| 137/137 [06:42<00:00,  2.94s/it]\n",
      "Epoch 7/25 - Validation: 100%|██████████| 137/137 [09:16<00:00,  4.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/25:\n",
      "Train Loss: 0.9478, Train Acc: 0.5463\n",
      "Val Loss: 1.1175, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25 - Training: 100%|██████████| 137/137 [06:34<00:00,  2.88s/it]\n",
      "Epoch 8/25 - Validation: 100%|██████████| 137/137 [08:58<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/25:\n",
      "Train Loss: 0.9392, Train Acc: 0.5545\n",
      "Val Loss: 1.1164, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25 - Training: 100%|██████████| 137/137 [06:20<00:00,  2.77s/it]\n",
      "Epoch 9/25 - Validation: 100%|██████████| 137/137 [09:03<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/25:\n",
      "Train Loss: 0.9890, Train Acc: 0.5087\n",
      "Val Loss: 1.1082, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25 - Training: 100%|██████████| 137/137 [06:36<00:00,  2.89s/it]\n",
      "Epoch 10/25 - Validation: 100%|██████████| 137/137 [08:52<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/25:\n",
      "Train Loss: 0.9382, Train Acc: 0.5518\n",
      "Val Loss: 1.1098, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25 - Training: 100%|██████████| 137/137 [06:35<00:00,  2.88s/it]\n",
      "Epoch 11/25 - Validation: 100%|██████████| 137/137 [08:53<00:00,  3.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/25:\n",
      "Train Loss: 0.9588, Train Acc: 0.5435\n",
      "Val Loss: 1.0982, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25 - Training: 100%|██████████| 137/137 [06:34<00:00,  2.88s/it]\n",
      "Epoch 12/25 - Validation: 100%|██████████| 137/137 [08:54<00:00,  3.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/25:\n",
      "Train Loss: 0.9481, Train Acc: 0.5390\n",
      "Val Loss: 1.0796, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25 - Training: 100%|██████████| 137/137 [06:49<00:00,  2.99s/it]\n",
      "Epoch 13/25 - Validation: 100%|██████████| 137/137 [09:36<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/25:\n",
      "Train Loss: 0.9659, Train Acc: 0.5353\n",
      "Val Loss: 1.0859, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25 - Training: 100%|██████████| 137/137 [06:51<00:00,  3.01s/it]\n",
      "Epoch 14/25 - Validation: 100%|██████████| 137/137 [09:04<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/25:\n",
      "Train Loss: 0.9678, Train Acc: 0.5252\n",
      "Val Loss: 1.0830, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25 - Training: 100%|██████████| 137/137 [06:55<00:00,  3.04s/it]\n",
      "Epoch 15/25 - Validation: 100%|██████████| 137/137 [09:34<00:00,  4.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/25:\n",
      "Train Loss: 0.9587, Train Acc: 0.5316\n",
      "Val Loss: 1.1164, Val Acc: 0.4491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25 - Training: 100%|██████████| 137/137 [06:44<00:00,  2.95s/it]\n",
      "Epoch 16/25 - Validation: 100%|██████████| 137/137 [09:31<00:00,  4.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/25:\n",
      "Train Loss: 0.9736, Train Acc: 0.5170\n",
      "Val Loss: 1.0537, Val Acc: 0.4491\n",
      "Early stopping triggered after 16 epochs\n",
      "Test değerlendirme hatası: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
      "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
      "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
      "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
      "\n",
      "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy.ndimage import rotate, zoom\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ---------------------------------------\n",
    "# CUDA ve CUDNN Ayarları\n",
    "# ---------------------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True  # Reproducibility için\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# Eğitim Parametreleri\n",
    "# ---------------------------------------\n",
    "num_epochs = 25\n",
    "batch_size = 8\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 1e-4\n",
    "accumulation_steps = 4\n",
    "patience = 15\n",
    "patch_size = (96, 96, 96)\n",
    "\n",
    "# Dinamik hiperparametre ayarlama fonksiyonu\n",
    "def adjust_hyperparameters(fold_results):\n",
    "    if not fold_results:\n",
    "        return {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'weight_decay': weight_decay,\n",
    "            'accumulation_steps': accumulation_steps\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        last_fold = fold_results[-1]\n",
    "        val_acc = last_fold['best_val_acc']\n",
    "        train_loss = last_fold.get('train_losses', [])[-1] if last_fold.get('train_losses') else None\n",
    "        \n",
    "        new_params = {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'weight_decay': weight_decay,\n",
    "            'accumulation_steps': accumulation_steps\n",
    "        }\n",
    "        \n",
    "        # Validation accuracy'ye göre learning rate ayarlama\n",
    "        if val_acc < 0.6:  # Düşük accuracy\n",
    "            new_params['learning_rate'] *= 1.5  # Learning rate'i artır\n",
    "        elif val_acc > 0.85:  # Yüksek accuracy\n",
    "            new_params['learning_rate'] *= 0.8  # Learning rate'i azalt\n",
    "        \n",
    "        # Overfitting kontrolü\n",
    "        if train_loss and last_fold.get('val_losses'):\n",
    "            train_loss = last_fold['train_losses'][-1]\n",
    "            val_loss = last_fold['val_losses'][-1]\n",
    "            if val_loss > train_loss * 1.2:  # Overfitting varsa\n",
    "                new_params['weight_decay'] *= 1.5  # Regularization'ı artır\n",
    "                new_params['batch_size'] = min(batch_size * 2, 32)  # Batch size'ı artır\n",
    "        \n",
    "        # Batch size sınırlamaları\n",
    "        new_params['batch_size'] = max(4, min(new_params['batch_size'], 32))\n",
    "        \n",
    "        print(\"\\nHiperparametre Ayarlamaları:\")\n",
    "        print(f\"Önceki Learning Rate: {learning_rate:.2e} -> Yeni: {new_params['learning_rate']:.2e}\")\n",
    "        print(f\"Önceki Batch Size: {batch_size} -> Yeni: {new_params['batch_size']}\")\n",
    "        print(f\"Önceki Weight Decay: {weight_decay:.2e} -> Yeni: {new_params['weight_decay']:.2e}\")\n",
    "        \n",
    "        return new_params\n",
    "    except Exception as e:\n",
    "        print(f\"Hiperparametre ayarlama hatası: {str(e)}\")\n",
    "        return {\n",
    "            'learning_rate': learning_rate,\n",
    "            'batch_size': batch_size,\n",
    "            'weight_decay': weight_decay,\n",
    "            'accumulation_steps': accumulation_steps\n",
    "        }\n",
    "\n",
    "# ---------------------------------------\n",
    "# Mixed Precision Training Setup\n",
    "# ---------------------------------------\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data Augmentation Functions\n",
    "# ---------------------------------------\n",
    "def random_rotation(img, max_angle=15):\n",
    "    try:\n",
    "        angle = random.uniform(-max_angle, max_angle)\n",
    "        return rotate(img, angle, axes=(0, 1), reshape=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Rotation hatası: {str(e)}\")\n",
    "        return img\n",
    "\n",
    "def random_zoom(img, min_zoom=0.9, max_zoom=1.1):\n",
    "    try:\n",
    "        zoom_factor = random.uniform(min_zoom, max_zoom)\n",
    "        return zoom(img, zoom_factor, order=1)\n",
    "    except Exception as e:\n",
    "        print(f\"Zoom hatası: {str(e)}\")\n",
    "        return img\n",
    "\n",
    "def random_flip(img, prob=0.5):\n",
    "    try:\n",
    "        if random.random() < prob:\n",
    "            return np.flip(img, axis=0)\n",
    "        return img\n",
    "    except Exception as e:\n",
    "        print(f\"Flip hatası: {str(e)}\")\n",
    "        return img\n",
    "\n",
    "def random_noise(img, mean=0, std=0.01):\n",
    "    try:\n",
    "        noise = np.random.normal(mean, std, img.shape)\n",
    "        return img + noise\n",
    "    except Exception as e:\n",
    "        print(f\"Noise hatası: {str(e)}\")\n",
    "        return img\n",
    "\n",
    "# ---------------------------------------\n",
    "# Dataset ve Model Tanımları\n",
    "# ---------------------------------------\n",
    "class ADNI_Dataset(Dataset):\n",
    "    def __init__(self, paths, labels, transform=None, is_training=True):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img = nib.load(self.paths[idx]).get_fdata()\n",
    "            \n",
    "            # Normalize\n",
    "            img = (img - img.mean()) / (img.std() + 1e-6)\n",
    "            \n",
    "            # Extract patch\n",
    "            center = np.array(img.shape) // 2\n",
    "            patch = extract_patch(img, center, patch_size)\n",
    "            \n",
    "            # Apply augmentations during training\n",
    "            if self.is_training:\n",
    "                patch = random_rotation(patch)\n",
    "                patch = random_zoom(patch)\n",
    "                patch = random_flip(patch)\n",
    "                patch = random_noise(patch)\n",
    "            \n",
    "            # Ensure patch is the correct size\n",
    "            if patch.shape != patch_size:\n",
    "                patch = zoom(patch, np.array(patch_size) / np.array(patch.shape), order=1)\n",
    "            \n",
    "            img_tensor = torch.from_numpy(patch).float().unsqueeze(0)\n",
    "            \n",
    "            if self.transform:\n",
    "                img_tensor = self.transform(img_tensor)\n",
    "            return img_tensor, self.labels[idx]\n",
    "        except Exception as e:\n",
    "            print(f\"Veri yükleme hatası (idx={idx}): {str(e)}\")\n",
    "            # Hata durumunda sıfır tensör döndür\n",
    "            return torch.zeros((1, *patch_size)), self.labels[idx]\n",
    "\n",
    "def extract_patch(img, center, size):\n",
    "    try:\n",
    "        # Ensure center is within bounds\n",
    "        center = np.clip(center, np.array(size) // 2, np.array(img.shape) - np.array(size) // 2)\n",
    "        \n",
    "        # Calculate start and end points\n",
    "        start = center - np.array(size) // 2\n",
    "        end = start + np.array(size)\n",
    "        \n",
    "        # Ensure start and end points are within bounds\n",
    "        start = np.clip(start, 0, np.array(img.shape) - np.array(size))\n",
    "        end = start + np.array(size)\n",
    "        \n",
    "        # Extract patch\n",
    "        patch = img[start[0]:end[0], start[1]:end[1], start[2]:end[2]]\n",
    "        \n",
    "        # Verify patch size\n",
    "        if patch.shape != size:\n",
    "            print(f\"Warning: Patch shape mismatch. Expected {size}, got {patch.shape}\")\n",
    "            # Resize patch to expected size if needed\n",
    "            patch = zoom(patch, np.array(size) / np.array(patch.shape), order=1)\n",
    "        \n",
    "        return patch\n",
    "    except Exception as e:\n",
    "        print(f\"Patch çıkarma hatası: {str(e)}\")\n",
    "        return np.zeros(size)\n",
    "\n",
    "class ImprovedCNN3D(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution with larger kernel\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv3d(1, 64, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm3d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout3d(0.1)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res1 = ResidualBlock(64, 128)\n",
    "        self.res2 = ResidualBlock(128, 256)\n",
    "        self.res3 = ResidualBlock(256, 512)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = SelfAttention(512)\n",
    "        \n",
    "        # Global pooling\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)\n",
    "        \n",
    "        # Classifier with more layers and dropout\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv3d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm3d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        \n",
    "        x = self.res1(x)\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        \n",
    "        x = self.res2(x)\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        \n",
    "        x = self.res3(x)\n",
    "        x = F.max_pool3d(x, 2)\n",
    "        \n",
    "        x = self.attention(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm3d(out_channels)\n",
    "        self.conv2 = nn.Conv3d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv3d(in_channels, out_channels, kernel_size=1),\n",
    "                nn.BatchNorm3d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.leaky_relu(self.bn1(self.conv1(x)), 0.2)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(residual)\n",
    "        out = F.leaky_relu(out, 0.2)\n",
    "        return out\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.query = nn.Conv3d(in_channels, in_channels//8, 1)\n",
    "        self.key = nn.Conv3d(in_channels, in_channels//8, 1)\n",
    "        self.value = nn.Conv3d(in_channels, in_channels, 1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, C, D, H, W = x.size()\n",
    "        \n",
    "        q = self.query(x).view(batch_size, -1, D*H*W)\n",
    "        k = self.key(x).view(batch_size, -1, D*H*W)\n",
    "        v = self.value(x).view(batch_size, -1, D*H*W)\n",
    "        \n",
    "        attention = F.softmax(torch.bmm(q.permute(0, 2, 1), k), dim=2)\n",
    "        out = torch.bmm(v, attention.permute(0, 2, 1))\n",
    "        out = out.view(batch_size, C, D, H, W)\n",
    "        \n",
    "        return self.gamma * out + x\n",
    "\n",
    "# ---------------------------------------\n",
    "# Veri Yükleme ve Hazırlama\n",
    "# ---------------------------------------\n",
    "try:\n",
    "    DATA_DIR = '/kaggle/input/adniunpreprocessed'\n",
    "    classes = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "\n",
    "    pattern = os.path.join(DATA_DIR, '**', '*.nii*')\n",
    "    all_files = glob.glob(pattern, recursive=True)\n",
    "    valid_paths = []\n",
    "    valid_labels = []\n",
    "\n",
    "    for fp in all_files:\n",
    "        parent = os.path.basename(os.path.dirname(fp))\n",
    "        if parent not in classes:\n",
    "            continue\n",
    "        if os.path.getsize(fp) == 0:\n",
    "            continue\n",
    "        try:\n",
    "            _ = nib.load(fp)\n",
    "            valid_paths.append(fp)\n",
    "            valid_labels.append(classes[parent])\n",
    "        except Exception as e:\n",
    "            print(f\"Dosya yükleme hatası ({fp}): {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    if not valid_paths:\n",
    "        raise ValueError(\"Geçerli veri bulunamadı!\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # K-Fold Cross Validation\n",
    "    # ---------------------------------------\n",
    "    n_splits = 2\n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    all_paths = np.array(valid_paths)\n",
    "    all_labels = np.array(valid_labels)\n",
    "\n",
    "    fold_results = []\n",
    "\n",
    "    for fold in range(n_splits):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # İlk fold'dan sonra hiperparametreleri ayarla\n",
    "        if fold > 0:\n",
    "            new_params = adjust_hyperparameters(fold_results)\n",
    "            learning_rate = new_params['learning_rate']\n",
    "            batch_size = new_params['batch_size']\n",
    "            weight_decay = new_params['weight_decay']\n",
    "            accumulation_steps = new_params['accumulation_steps']\n",
    "        \n",
    "        # Data split with stratification\n",
    "        train_idx, val_idx = list(kfold.split(all_paths, all_labels))[fold]\n",
    "        paths_train = all_paths[train_idx]\n",
    "        labels_train = all_labels[train_idx]\n",
    "        paths_val = all_paths[val_idx]\n",
    "        labels_val = all_labels[val_idx]\n",
    "        \n",
    "        # Print class distribution for this fold\n",
    "        print(\"\\nTrain set class distribution:\")\n",
    "        print(Counter(labels_train))\n",
    "        print(\"\\nValidation set class distribution:\")\n",
    "        print(Counter(labels_val))\n",
    "        \n",
    "        # Calculate class weights for this fold\n",
    "        counter_train = Counter(labels_train)\n",
    "        total_train = sum(counter_train.values())\n",
    "        class_weights = [np.sqrt(total_train / (len(counter_train) * count)) for count in counter_train.values()]\n",
    "        class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)\n",
    "        print(\"\\nClass weights for this fold:\", class_weights)\n",
    "        \n",
    "        # Create weighted sampler for training\n",
    "        sample_weights = [class_weights[label] for label in labels_train]\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=sample_weights,\n",
    "            num_samples=len(sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "        \n",
    "        # Dataset and DataLoader with weighted sampling\n",
    "        train_ds = ADNI_Dataset(paths_train, labels_train)\n",
    "        val_ds = ADNI_Dataset(paths_val, labels_val)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=2,\n",
    "            pin_memory=True\n",
    "        )\n",
    "        \n",
    "        # Model, Loss and Optimizer\n",
    "        model = ImprovedCNN3D(num_classes=3).to(DEVICE)\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        \n",
    "        # Cosine annealing with warm restarts\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "            optimizer,\n",
    "            T_0=10,\n",
    "            T_mult=2,\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "        \n",
    "        # Use weighted loss function with label smoothing\n",
    "        cls_loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor, label_smoothing=0.1)\n",
    "        \n",
    "        # Training\n",
    "        best_val_acc = 0.0\n",
    "        epochs_no_improve = 0\n",
    "        train_losses, val_losses = [], []\n",
    "        train_accs, val_accs = [], []\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            total_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i, (imgs, labels) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
    "                try:\n",
    "                    imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "                    labels = labels.to(DEVICE, non_blocking=True)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        outputs = model(imgs)\n",
    "                        loss = cls_loss_fn(outputs, labels) / accumulation_steps\n",
    "                    \n",
    "                    scaler.scale(loss).backward()\n",
    "                    \n",
    "                    if (i + 1) % accumulation_steps == 0:\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                        optimizer.zero_grad()\n",
    "                    \n",
    "                    total_loss += loss.item() * accumulation_steps\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    correct += (preds == labels).sum().item()\n",
    "                    total += labels.size(0)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Training batch hatası (epoch {epoch+1}, batch {i}): {str(e)}\")\n",
    "                    continue\n",
    "            \n",
    "            train_loss = total_loss / len(train_loader)\n",
    "            train_acc = correct / total\n",
    "            train_losses.append(train_loss)\n",
    "            train_accs.append(train_acc)\n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
    "                    try:\n",
    "                        imgs = imgs.to(DEVICE)\n",
    "                        labels = labels.to(DEVICE)\n",
    "                        \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(imgs)\n",
    "                            loss = cls_loss_fn(outputs, labels)\n",
    "                        \n",
    "                        val_loss += loss.item()\n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        correct += (preds == labels).sum().item()\n",
    "                        total += labels.size(0)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Validation batch hatası (epoch {epoch+1}): {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = correct / total\n",
    "            val_losses.append(val_loss)\n",
    "            val_accs.append(val_acc)\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "            \n",
    "            scheduler.step()\n",
    "            \n",
    "            if val_acc > best_val_acc + 1e-4:\n",
    "                best_val_acc = val_acc\n",
    "                epochs_no_improve = 0\n",
    "                try:\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict(),\n",
    "                        'best_val_acc': best_val_acc,\n",
    "                        'class_weights': class_weights,\n",
    "                        'fold': fold + 1\n",
    "                    }, f'best_model_fold_{fold+1}.pth')\n",
    "                    print(\"--Yeni en iyi model kaydedildi--\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Model kaydetme hatası: {str(e)}\")\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "            \n",
    "            # Memory cleanup\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Test evaluation for this fold\n",
    "        try:\n",
    "            checkpoint = torch.load(f'best_model_fold_{fold+1}.pth')\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model.eval()\n",
    "            \n",
    "            all_targets = []\n",
    "            all_preds = []\n",
    "            all_probs = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in tqdm(val_loader, desc=\"Testing\"):\n",
    "                    try:\n",
    "                        imgs = imgs.to(DEVICE)\n",
    "                        labels = labels.to(DEVICE)\n",
    "                        \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            outputs = model(imgs)\n",
    "                            probs = F.softmax(outputs, dim=1)\n",
    "                        \n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        all_targets.extend(labels.cpu().numpy())\n",
    "                        all_preds.extend(preds.cpu().numpy())\n",
    "                        all_probs.extend(probs.cpu().numpy())\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Test batch hatası: {str(e)}\")\n",
    "                        continue\n",
    "            \n",
    "            # Calculate metrics for this fold\n",
    "            fold_acc = np.mean(np.array(all_preds) == np.array(all_targets))\n",
    "            fold_report = classification_report(all_targets, all_preds, target_names=[\"CN\", \"MCI\", \"AD\"], output_dict=True)\n",
    "            \n",
    "            # Save fold results\n",
    "            fold_results.append({\n",
    "                'fold': fold + 1,\n",
    "                'accuracy': fold_acc,\n",
    "                'classification_report': fold_report,\n",
    "                'predictions': all_preds,\n",
    "                'targets': all_targets,\n",
    "                'probabilities': all_probs,\n",
    "                'class_weights': class_weights,\n",
    "                'best_val_acc': best_val_acc,\n",
    "                'best_epoch': checkpoint['epoch'],\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'train_accs': train_accs,\n",
    "                'val_accs': val_accs\n",
    "            })\n",
    "            \n",
    "            # Save fold results to file\n",
    "            try:\n",
    "                with open(f'fold_{fold+1}_results.txt', 'w') as f:\n",
    "                    f.write(f\"Fold {fold+1} Results:\\n\")\n",
    "                    f.write(f\"Best Validation Accuracy: {best_val_acc:.4f}\\n\")\n",
    "                    f.write(f\"Best Epoch: {checkpoint['epoch']}\\n\")\n",
    "                    f.write(f\"Final Test Accuracy: {fold_acc:.4f}\\n\")\n",
    "                    f.write(f\"Class Weights Used: {class_weights}\\n\\n\")\n",
    "                    f.write(\"Classification Report:\\n\")\n",
    "                    f.write(classification_report(all_targets, all_preds, target_names=[\"CN\", \"MCI\", \"AD\"]))\n",
    "            except Exception as e:\n",
    "                print(f\"Sonuç dosyası yazma hatası: {str(e)}\")\n",
    "            \n",
    "            # Plot confusion matrix for this fold\n",
    "            try:\n",
    "                cm = confusion_matrix(all_targets, all_preds)\n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(cm, annot=True, fmt='d', xticklabels=[\"CN\", \"MCI\", \"AD\"], yticklabels=[\"CN\", \"MCI\", \"AD\"])\n",
    "                plt.xlabel(\"Predicted\")\n",
    "                plt.ylabel(\"True\")\n",
    "                plt.title(f\"Confusion Matrix - Fold {fold+1}\")\n",
    "                plt.savefig(f'confusion_matrix_fold_{fold+1}.png')\n",
    "                plt.close()\n",
    "            except Exception as e:\n",
    "                print(f\"Confusion matrix çizim hatası: {str(e)}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Test değerlendirme hatası: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save all fold results to a single file\n",
    "    try:\n",
    "        with open('all_folds_results.txt', 'w') as f:\n",
    "            f.write(\"All Folds Results:\\n\\n\")\n",
    "            for fold_result in fold_results:\n",
    "                f.write(f\"Fold {fold_result['fold']}:\\n\")\n",
    "                f.write(f\"Best Validation Accuracy: {fold_result['best_val_acc']:.4f}\\n\")\n",
    "                f.write(f\"Best Epoch: {fold_result['best_epoch']}\\n\")\n",
    "                f.write(f\"Final Test Accuracy: {fold_result['accuracy']:.4f}\\n\")\n",
    "                f.write(f\"Class Weights Used: {fold_result['class_weights']}\\n\")\n",
    "                f.write(\"Classification Report:\\n\")\n",
    "                f.write(classification_report(fold_result['targets'], fold_result['predictions'], target_names=[\"CN\", \"MCI\", \"AD\"]))\n",
    "                f.write(\"\\n\" + \"=\"*50 + \"\\n\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tüm sonuçları kaydetme hatası: {str(e)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Genel hata: {str(e)}\")\n",
    "finally:\n",
    "    # Final cleanup\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7573927,
     "sourceId": 12036716,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30684.660582,
   "end_time": "2025-06-11T16:35:47.103570",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-11T08:04:22.442988",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
