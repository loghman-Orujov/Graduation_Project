{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6bea2dd",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-12T01:25:58.634988Z",
     "iopub.status.busy": "2025-06-12T01:25:58.634763Z",
     "iopub.status.idle": "2025-06-12T02:49:35.623298Z",
     "shell.execute_reply": "2025-06-12T02:49:35.622311Z"
    },
    "papermill": {
     "duration": 5016.996273,
     "end_time": "2025-06-12T02:49:35.627611",
     "exception": false,
     "start_time": "2025-06-12T01:25:58.631338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Total sample sayısı: 2182\n",
      "Sınıf dağılımı: {'CN': 748, 'MCI': 981, 'AD': 453}\n",
      "Train / Test sayıları: 1745/437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1 Epoch Eğitim: 100%|██████████| 873/873 [08:58<00:00,  1.62it/s]\n",
      "Özellik Çıkarma: 100%|██████████| 873/873 [08:35<00:00,  1.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Eğitiliyor: Random Forest\n",
      "En iyi parametreler: {'max_depth': 15, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Random Forest - CV Accuracy: 0.6458 ± 0.0003\n",
      "CV Precision: 0.6564\n",
      "CV Recall: 0.6458\n",
      "CV F1: 0.6422\n",
      "\n",
      "Eğitiliyor: Gradient Boosting\n",
      "En iyi parametreler: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
      "Gradient Boosting - CV Accuracy: 0.6756 ± 0.0039\n",
      "CV Precision: 0.6778\n",
      "CV Recall: 0.6756\n",
      "CV F1: 0.6746\n",
      "\n",
      "Eğitiliyor: SVM\n",
      "En iyi parametreler: {'C': 10, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "SVM - CV Accuracy: 0.5948 ± 0.0223\n",
      "CV Precision: 0.6316\n",
      "CV Recall: 0.5948\n",
      "CV F1: 0.5813\n",
      "\n",
      "Eğitiliyor: KNN\n",
      "En iyi parametreler: {'metric': 'euclidean', 'n_neighbors': 3}\n",
      "KNN - CV Accuracy: 0.6264 ± 0.0061\n",
      "CV Precision: 0.6293\n",
      "CV Recall: 0.6264\n",
      "CV F1: 0.6255\n",
      "\n",
      "Eğitiliyor: Logistic Regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En iyi parametreler: {'C': 10, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression - CV Accuracy: 0.6126 ± 0.0035\n",
      "CV Precision: 0.6318\n",
      "CV Recall: 0.6126\n",
      "CV F1: 0.6081\n",
      "\n",
      "Eğitiliyor: Naive Bayes\n",
      "Naive Bayes - CV Accuracy: 0.4069 ± 0.0169\n",
      "CV Precision: 0.4541\n",
      "CV Recall: 0.4069\n",
      "CV F1: 0.4006\n",
      "\n",
      "Eğitiliyor: Decision Tree\n",
      "En iyi parametreler: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2}\n",
      "Decision Tree - CV Accuracy: 0.5324 ± 0.0106\n",
      "CV Precision: 0.5410\n",
      "CV Recall: 0.5324\n",
      "CV F1: 0.5322\n",
      "\n",
      "Eğitiliyor: Neural Network\n",
      "En iyi parametreler: {'alpha': 0.001, 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}\n",
      "Neural Network - CV Accuracy: 0.5989 ± 0.0360\n",
      "CV Precision: 0.5989\n",
      "CV Recall: 0.5989\n",
      "CV F1: 0.5979\n",
      "\n",
      "ML Algoritmaları Sonuçları:\n",
      "                 Model  CV Accuracy  CV Accuracy Std  CV Precision  CV Recall  \\\n",
      "0        Random Forest     0.645845         0.000287      0.656389   0.645845   \n",
      "1    Gradient Boosting     0.675646         0.003926      0.677772   0.675646   \n",
      "2                  SVM     0.594830         0.022255      0.631586   0.594830   \n",
      "3                  KNN     0.626365         0.006081      0.629317   0.626365   \n",
      "4  Logistic Regression     0.612608         0.003508      0.631822   0.612608   \n",
      "5          Naive Bayes     0.406885         0.016946      0.454058   0.406885   \n",
      "6        Decision Tree     0.532387         0.010591      0.541038   0.532387   \n",
      "7       Neural Network     0.598875         0.035976      0.598937   0.598875   \n",
      "\n",
      "      CV F1  \n",
      "0  0.642216  \n",
      "1  0.674585  \n",
      "2  0.581251  \n",
      "3  0.625503  \n",
      "4  0.608130  \n",
      "5  0.400620  \n",
      "6  0.532230  \n",
      "7  0.597906  \n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Gerekli Kütüphaneler\n",
    "# ---------------------------------------\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torchvision.models.video as video_models\n",
    "from torch.amp import autocast, GradScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import joblib\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------------------------\n",
    "# CUDA ve CUDNN Ayarları\n",
    "# ---------------------------------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1) Dataset Dizini ve Sınıf Etiketleri\n",
    "# ---------------------------------------\n",
    "DATA_DIR = '/kaggle/input/adniunpreprocessed'  # Ham NIfTI klasörünüz\n",
    "classes = {'CN': 0, 'MCI': 1, 'AD': 2}\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2) Dosya Toplama ve Filtreleme\n",
    "# ---------------------------------------\n",
    "pattern = os.path.join(DATA_DIR, '**', '*.nii*')\n",
    "all_files = glob.glob(pattern, recursive=True)\n",
    "valid_paths, valid_labels = [], []\n",
    "\n",
    "for fp in all_files:\n",
    "    parent = os.path.basename(os.path.dirname(fp))\n",
    "    if parent not in classes:\n",
    "        continue\n",
    "    if os.path.getsize(fp) == 0:\n",
    "        continue\n",
    "    valid_paths.append(fp)\n",
    "    valid_labels.append(classes[parent])\n",
    "\n",
    "# İkinci filtre: sadece açılabilen NIfTI\n",
    "clean_paths, clean_labels = [], []\n",
    "for path, label in zip(valid_paths, valid_labels):\n",
    "    try:\n",
    "        _ = nib.load(path)\n",
    "        clean_paths.append(path)\n",
    "        clean_labels.append(label)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "valid_paths  = clean_paths\n",
    "valid_labels = clean_labels\n",
    "print(f\"Total sample sayısı: {len(valid_paths)}\")\n",
    "print(\"Sınıf dağılımı:\", {c: valid_labels.count(classes[c]) for c in classes})\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3) Train/Test Split\n",
    "# ---------------------------------------\n",
    "paths_train, paths_test, labels_train, labels_test = train_test_split(\n",
    "    valid_paths, valid_labels, test_size=0.20, stratify=valid_labels, random_state=42\n",
    ")\n",
    "print(f\"Train / Test sayıları: {len(paths_train)}/{len(paths_test)}\")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 4) Model Sınıfı\n",
    "# ---------------------------------------\n",
    "class DenseLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
    "        self.conv1 = nn.Conv3d(in_channels, bn_size * growth_rate, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm3d(bn_size * growth_rate)\n",
    "        self.conv2 = nn.Conv3d(bn_size * growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n",
    "        self.drop_rate = drop_rate\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.bn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv1(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.drop_rate > 0:\n",
    "            out = F.dropout(out, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, out], 1)\n",
    "\n",
    "class DenseBlock3D(nn.Module):\n",
    "    def __init__(self, num_layers, in_channels, growth_rate, bn_size, drop_rate):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.layers.append(DenseLayer3D(in_channels + i * growth_rate, growth_rate, bn_size, drop_rate))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class TransitionLayer3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.bn = nn.BatchNorm3d(in_channels)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AvgPool3d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "class DenseNet3DClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=3, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0.5):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv3d(1, num_init_features, kernel_size=7, stride=2, padding=3, bias=False),\n",
    "            nn.BatchNorm3d(num_init_features),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool3d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        # Dense blocks\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = DenseBlock3D(\n",
    "                num_layers=num_layers,\n",
    "                in_channels=num_features,\n",
    "                growth_rate=growth_rate,\n",
    "                bn_size=bn_size,\n",
    "                drop_rate=drop_rate\n",
    "            )\n",
    "            self.features.add_module(f'denseblock{i+1}', block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            \n",
    "            if i != len(block_config) - 1:\n",
    "                trans = TransitionLayer3D(\n",
    "                    in_channels=num_features,\n",
    "                    out_channels=num_features // 2\n",
    "                )\n",
    "                self.features.add_module(f'transition{i+1}', trans)\n",
    "                num_features = num_features // 2\n",
    "        \n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm3d(num_features))\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool3d((1, 1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=drop_rate),\n",
    "            nn.Linear(num_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = self.classifier(features)\n",
    "        return out\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        features = self.features(x)\n",
    "        features = self.classifier[0](features)  # ReLU\n",
    "        features = self.classifier[1](features)  # AdaptiveAvgPool3d\n",
    "        features = self.classifier[2](features)  # Flatten\n",
    "        return features\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5) Dataset Sınıfı\n",
    "# ---------------------------------------\n",
    "class ADNI_ResNet3D_Dataset(Dataset):\n",
    "    def __init__(self, paths, labels, \n",
    "                 target_depth=64, target_height=112, target_width=112,\n",
    "                 is_training=True):\n",
    "        self.paths = paths\n",
    "        self.labels = labels\n",
    "        self.d = target_depth\n",
    "        self.h = target_height\n",
    "        self.w = target_width\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1) Load NIfTI\n",
    "        path = self.paths[idx]\n",
    "        img = nib.load(path).get_fdata().astype(np.float32)\n",
    "\n",
    "        # 2) Robust normalization (%1-%99 persentil arasını [0,1])\n",
    "        p1, p99 = np.percentile(img, (1, 99))\n",
    "        img = np.clip(img, p1, p99)\n",
    "        img = (img - p1) / (p99 - p1 + 1e-6)\n",
    "\n",
    "        # 3) Center-crop veya pad 3D hacim → target (D x H x W)\n",
    "        D, H, W = img.shape\n",
    "        \n",
    "        # Depth (z ekseni) orta bölgeden target_depth al\n",
    "        cd = D // 2\n",
    "        hd = self.d // 2\n",
    "        start_d = max(cd - hd, 0)\n",
    "        end_d = start_d + self.d\n",
    "        if end_d > D:\n",
    "            end_d = D\n",
    "            start_d = D - self.d\n",
    "        patch = img[start_d:end_d, :, :]\n",
    "\n",
    "        # Yükseklik ve genişlik için merkezden al veya pad\n",
    "        ch = H // 2\n",
    "        hh = self.h // 2\n",
    "        start_h = max(ch - hh, 0)\n",
    "        end_h = start_h + self.h\n",
    "        if end_h > H:\n",
    "            end_h = H\n",
    "            start_h = H - self.h\n",
    "        patch = patch[:, start_h:end_h, :]\n",
    "\n",
    "        cw = W // 2\n",
    "        hw = self.w // 2\n",
    "        start_w = max(cw - hw, 0)\n",
    "        end_w = start_w + self.w\n",
    "        if end_w > W:\n",
    "            end_w = W\n",
    "            start_w = W - self.w\n",
    "        patch = patch[:, :, start_w:end_w]\n",
    "\n",
    "        # Eğer patch boyutları eksikse pad et\n",
    "        pd, ph, pw = patch.shape\n",
    "        if pd != self.d or ph != self.h or pw != self.w:\n",
    "            padded = np.zeros((self.d, self.h, self.w), dtype=np.float32)\n",
    "            sd = (self.d - pd) // 2\n",
    "            sh = (self.h - ph) // 2\n",
    "            sw = (self.w - pw) // 2\n",
    "            padded[sd:sd+pd, sh:sh+ph, sw:sw+pw] = patch\n",
    "            patch = padded\n",
    "\n",
    "        # 4) Tensor'a dönüştür ve kanal ekseni ekle: (1, D, H, W)\n",
    "        img_tensor = torch.from_numpy(patch).unsqueeze(0)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return img_tensor, label\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6) Dataset ve DataLoader\n",
    "# ---------------------------------------\n",
    "train_ds = ADNI_ResNet3D_Dataset(\n",
    "    paths_train, labels_train,\n",
    "    target_depth=64, target_height=112, target_width=112,\n",
    "    is_training=True\n",
    ")\n",
    "\n",
    "test_ds = ADNI_ResNet3D_Dataset(\n",
    "    paths_test, labels_test,\n",
    "    target_depth=64, target_height=112, target_width=112,\n",
    "    is_training=False\n",
    ")\n",
    "\n",
    "batch_size = 2\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=batch_size,\n",
    "    shuffle=True, num_workers=2,\n",
    "    pin_memory=True, prefetch_factor=2\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=batch_size,\n",
    "    shuffle=False, num_workers=2,\n",
    "    pin_memory=True, prefetch_factor=2\n",
    ")\n",
    "\n",
    "# ---------------------------------------\n",
    "# 8) Özellik Çıkarma ve ML Algoritmaları\n",
    "# ---------------------------------------\n",
    "def extract_features_and_train_ml(model, data_loader, device):\n",
    "    model.eval()\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Özellik çıkarma sürecini optimize et\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(data_loader, desc=\"Özellik Çıkarma\"):\n",
    "            imgs = imgs.to(device)\n",
    "            features = model.extract_features(imgs)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "            all_labels.append(labels.numpy())\n",
    "    \n",
    "    X = np.vstack(all_features)\n",
    "    y = np.concatenate(all_labels)\n",
    "    \n",
    "    # Veri ölçeklendirme\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Sınıf ağırlıklarını hesapla\n",
    "    class_counts = np.bincount(y)\n",
    "    class_weights = len(y) / (len(np.unique(y)) * class_counts)\n",
    "    sample_weights = class_weights[y]\n",
    "    \n",
    "    # ML Algoritmaları ve hiperparametreler\n",
    "    ml_models = {\n",
    "        'Random Forest': {\n",
    "            'model': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],  # Azaltıldı\n",
    "                'max_depth': [10, 15],\n",
    "                'min_samples_split': [2]\n",
    "            }\n",
    "        },\n",
    "        'Gradient Boosting': {\n",
    "            'model': GradientBoostingClassifier(random_state=42),\n",
    "            'params': {\n",
    "                'n_estimators': [100, 200],  # Azaltıldı\n",
    "                'learning_rate': [0.1],\n",
    "                'max_depth': [3, 5]\n",
    "            }\n",
    "        },\n",
    "        'SVM': {\n",
    "            'model': SVC(probability=True, random_state=42, class_weight='balanced'),\n",
    "            'params': {\n",
    "                'C': [1, 10],\n",
    "                'kernel': ['rbf'],\n",
    "                'gamma': ['scale']\n",
    "            }\n",
    "        },\n",
    "        'KNN': {\n",
    "            'model': KNeighborsClassifier(weights='distance'),\n",
    "            'params': {\n",
    "                'n_neighbors': [3, 5],\n",
    "                'metric': ['euclidean']\n",
    "            }\n",
    "        },\n",
    "        'Logistic Regression': {\n",
    "            'model': LogisticRegression(\n",
    "                max_iter=1000,  # Azaltıldı\n",
    "                solver='saga',\n",
    "                random_state=42,\n",
    "                class_weight='balanced',\n",
    "                n_jobs=1\n",
    "            ),\n",
    "            'params': {\n",
    "                'C': [1, 10],\n",
    "                'penalty': ['l2']\n",
    "            }\n",
    "        },\n",
    "        'Naive Bayes': {\n",
    "            'model': GaussianNB(),\n",
    "            'params': {}\n",
    "        },\n",
    "        'Decision Tree': {\n",
    "            'model': DecisionTreeClassifier(random_state=42, class_weight='balanced'),\n",
    "            'params': {\n",
    "                'max_depth': [5, 10],\n",
    "                'min_samples_split': [2],\n",
    "                'min_samples_leaf': [1]\n",
    "            }\n",
    "        },\n",
    "        'Neural Network': {\n",
    "            'model': MLPClassifier(\n",
    "                max_iter=1000,  # Azaltıldı\n",
    "                random_state=42,\n",
    "                solver='adam',\n",
    "                early_stopping=True\n",
    "            ),\n",
    "            'params': {\n",
    "                'hidden_layer_sizes': [(100,)],\n",
    "                'alpha': [0.001],\n",
    "                'learning_rate': ['adaptive']\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save the scaler for later use\n",
    "    try:\n",
    "        joblib.dump(scaler, 'feature_scaler.joblib')\n",
    "    except Exception as e:\n",
    "        print(f\"Scaler kaydedilemedi: {str(e)}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Cross-validation için StratifiedKFold - kat sayısı azaltıldı\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Her model için eğitim ve değerlendirme\n",
    "    for name, model_info in ml_models.items():\n",
    "        print(f\"\\nEğitiliyor: {name}\")\n",
    "        \n",
    "        try:\n",
    "            # Hiperparametre optimizasyonu\n",
    "            if model_info['params']:\n",
    "                grid_search = GridSearchCV(\n",
    "                    model_info['model'],\n",
    "                    model_info['params'],\n",
    "                    cv=skf,\n",
    "                    scoring='f1_weighted',\n",
    "                    n_jobs=1\n",
    "                )\n",
    "                if name in ['KNN', 'Neural Network']:\n",
    "                    grid_search.fit(X, y)\n",
    "                else:\n",
    "                    grid_search.fit(X, y, sample_weight=sample_weights)\n",
    "                best_model = grid_search.best_estimator_\n",
    "                print(f\"En iyi parametreler: {grid_search.best_params_}\")\n",
    "            else:\n",
    "                best_model = model_info['model']\n",
    "                if name in ['KNN', 'Neural Network']:\n",
    "                    best_model.fit(X, y)\n",
    "                else:\n",
    "                    best_model.fit(X, y, sample_weight=sample_weights)\n",
    "            \n",
    "            # Cross-validation sonuçları\n",
    "            cv_scores = []\n",
    "            cv_precision = []\n",
    "            cv_recall = []\n",
    "            cv_f1 = []\n",
    "            \n",
    "            for train_idx, val_idx in skf.split(X, y):\n",
    "                X_train, X_val = X[train_idx], X[val_idx]\n",
    "                y_train, y_val = y[train_idx], y[val_idx]\n",
    "                sample_weights_train = sample_weights[train_idx]\n",
    "                \n",
    "                if name in ['KNN', 'Neural Network']:\n",
    "                    best_model.fit(X_train, y_train)\n",
    "                else:\n",
    "                    best_model.fit(X_train, y_train, sample_weight=sample_weights_train)\n",
    "                y_pred = best_model.predict(X_val)\n",
    "                \n",
    "                cv_scores.append(accuracy_score(y_val, y_pred))\n",
    "                precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, average='weighted')\n",
    "                cv_precision.append(precision)\n",
    "                cv_recall.append(recall)\n",
    "                cv_f1.append(f1)\n",
    "            \n",
    "            # Modeli kaydet\n",
    "            try:\n",
    "                joblib.dump(best_model, f'{name.lower().replace(\" \", \"_\")}_model.joblib')\n",
    "            except Exception as e:\n",
    "                print(f\"Model kaydedilemedi: {str(e)}\")\n",
    "            \n",
    "            # Sonuçları kaydet\n",
    "            results[name] = {\n",
    "                'CV Accuracy': np.mean(cv_scores),\n",
    "                'CV Accuracy Std': np.std(cv_scores),\n",
    "                'CV Precision': np.mean(cv_precision),\n",
    "                'CV Recall': np.mean(cv_recall),\n",
    "                'CV F1': np.mean(cv_f1),\n",
    "                'Best Model': best_model\n",
    "            }\n",
    "            \n",
    "            print(f\"{name} - CV Accuracy: {results[name]['CV Accuracy']:.4f} ± {results[name]['CV Accuracy Std']:.4f}\")\n",
    "            print(f\"CV Precision: {results[name]['CV Precision']:.4f}\")\n",
    "            print(f\"CV Recall: {results[name]['CV Recall']:.4f}\")\n",
    "            print(f\"CV F1: {results[name]['CV F1']:.4f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{name} modeli eğitilirken hata oluştu: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Sonuçları DataFrame'e dönüştür ve kaydet\n",
    "    results_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'CV Accuracy': [results[name]['CV Accuracy'] for name in results.keys()],\n",
    "        'CV Accuracy Std': [results[name]['CV Accuracy Std'] for name in results.keys()],\n",
    "        'CV Precision': [results[name]['CV Precision'] for name in results.keys()],\n",
    "        'CV Recall': [results[name]['CV Recall'] for name in results.keys()],\n",
    "        'CV F1': [results[name]['CV F1'] for name in results.keys()]\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        results_df.to_csv('ml_results.csv', index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Sonuçlar kaydedilemedi: {str(e)}\")\n",
    "    \n",
    "    # Confusion matrix görselleştirme\n",
    "    for name in results.keys():\n",
    "        try:\n",
    "            best_model = results[name]['Best Model']\n",
    "            y_pred = best_model.predict(X)\n",
    "            cm = confusion_matrix(y, y_pred)\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'Confusion Matrix - {name}')\n",
    "            plt.ylabel('True Label')\n",
    "            plt.xlabel('Predicted Label')\n",
    "            plt.savefig(f'{name.lower().replace(\" \", \"_\")}_confusion_matrix.png')\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(f\"{name} için confusion matrix oluşturulamadı: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# ---------------------------------------\n",
    "# 9) Model Eğitimi ve ML\n",
    "# ---------------------------------------\n",
    "def train_and_extract_features():\n",
    "    model = DenseNet3DClassifier(num_classes=3).to(DEVICE)\n",
    "    \n",
    "    # Sadece 1 epoch için eğitim\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "    cls_loss_fn = CrossEntropyLoss()\n",
    "    \n",
    "    # 1 epoch eğitim\n",
    "    model.train()\n",
    "    for imgs, labels in tqdm(train_loader, desc=\"1 Epoch Eğitim\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(imgs)\n",
    "        loss = cls_loss_fn(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Özellik çıkarma ve ML eğitimi\n",
    "    results_df = extract_features_and_train_ml(model, train_loader, DEVICE)\n",
    "    print(\"\\nML Algoritmaları Sonuçları:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Eğitimi başlat\n",
    "if __name__ == \"__main__\":\n",
    "    results = train_and_extract_features()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7573927,
     "sourceId": 12036716,
     "sourceType": "datasetVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5024.353839,
   "end_time": "2025-06-12T02:49:38.510988",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-12T01:25:54.157149",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
